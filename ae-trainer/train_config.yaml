# config.yaml
# model:
encoder: bert-large
decoder: qwen2-1.5b

# hparams:
learning_rate: 1.e-4
temp_loss_weight: 0
contrast_loss_weight: 0.1
rec_loss_weight: 1
teacher_forcing_ratio: 0.5

# training:
tr-precision: "bf16-mixed" # or bf16-mixed
devices: "4,5,6,7" # GPU devices

accumulate_grad_batches: 2
gradient_clip_val: 1.0
max_epochs: 20
mgpu-strategy: "auto" # or "ddp" for distributed training

# data:
data_path: "data/GSM8k-CoT_50000.pickle"
num_workers: 2
train_ratio: 0.8
batch_size: 64
max_length: 48

# wandb: 814817
wandb-project: "lcm-vae"
checkpoint: "checkpoints/last.ckpt"
model_dir: "checkpoints/"
